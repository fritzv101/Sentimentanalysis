import pandas as pd
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('all')

df = pd.read_csv('/content/drive/MyDrive/AI4ALL/training.1600000.processed.noemoticon.csv', encoding="latin1", header=None)
df.head()

statements = df.iloc[:, 5]
statements

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]
    processed_text = ' '.join(lemmatized_tokens)
    return processed_text # processed_text is returned

statements = df.iloc[:, 5].apply(preprocess_text) # processed text
ratings = df.iloc[:, 0] # 0 is negative, 4 is positive, 2 is neutral. this is the original rating
# statements
# ratings

analyzer = SentimentIntensityAnalyzer() # using the default NLP function
def get_sentiment(text):  # get the sentiment score of the text
    scores = analyzer.polarity_scores(text)
    sentiment = []
    # sentiment is an array where if the score is 'pos' 2 is appended, more 'neg' 0 is appended, else 'neu' is 1
    if scores['pos'] > scores['neg'] and scores['pos'] > scores['neu']:
      sentiment.append(2)
    elif scores['neg'] > 0:
      sentiment.append(0)
    else:
      sentiment.append(1)
    return sentiment
sentiments_own = [(get_sentiment(i), i) for i in statements] # for all processed text, get the sentiment. sentiment_own is an array of sentiments that are determined by the NLP analyzer
# sentiments_own

ratings_original = [] # ratings_original is an array of sentiments for the unprocessed original ratings
for i in ratings: # all the unprocessed ratings
  if i == 0:
    ratings_original.append(0)
  elif i == 4:
    ratings_original.append(2)
  else:
    ratings_original.append(1)
# ratings_original

total = 0
for index, value in enumerate(ratings_original):
  #print(ratings_original[index], sentiments_own[index])
  if ratings_original[index] == sentiments_own[index][0][0]:
    total += 1 # if the processed ratings
print("Accuracy is:", total / 1000)
# the accuracy is 0.515 out of 3 choices, positive, negative, and neutral

!pip install -q transformers
import random
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

tokenizer = AutoTokenizer.from_pretrained("DunnBC22/bert-base-uncased-Twitter_Sentiment_Analysis_v2")
model = AutoModelForSequenceClassification.from_pretrained("DunnBC22/bert-base-uncased-Twitter_Sentiment_Analysis_v2")

sentiment_pipeline = pipeline("text-classification", model=model, tokenizer=tokenizer)
sample_texts = df.iloc[:1000, 5]
sentiments = []
for text in sample_texts:
    result = sentiment_pipeline(text)[0]
    sentiments.append({"text": text, "sentiment": result["label"], "score": result["score"]})

total = 0
hugging_ratings = []
for i in sentiments:
  if i['sentiment'] == 'positive':
    hugging_ratings.append(4)
  if i['sentiment'] == 'negative':
    hugging_ratings.append(0)
  else:
    hugging_ratings.append(2)

for index, value in enumerate(ratings):
  if ratings[index] == hugging_ratings[index]:
    total += 1

print("Acc is:", total / 1000)

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import re
import string
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense
from tensorflow.keras.models import Sequential
from keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score

df = pd.read_csv('/content/drive/MyDrive/AI4ALL/training.1600000.processed.noemoticon.csv', encoding="latin1", header=None)
df.head()
#Index(['target', 'ids', 'date', 'flag', 'user', 'text'], dtype='object')
df.columns
# 1600000 Tweets and 6 columns in this Dataset
df.shape

df.info()
df.describe()
df.duplicated().sum()
# Lets Explore the id column
df_ids=df[1].min()
print("The Minimum ids are:",df_ids)
df_ids1=df[1].max()
print("The Maximum ids are:",df_ids1)
df_ids2=df[1].sum()
print("The sum of ids are:",df_ids2)

# First, let's get the value counts of the 'target' column
df_1 = df[0].value_counts()

# Create a DataFrame with the counts
df_target = pd.DataFrame(df_1)

# Reset the index to have the 'target' values as a column
df_target = df_target.reset_index()

# Rename the columns
df_target.columns = ['target', 'count']

# Filter the DataFrame to get counts of positive (1) and negative (0) tweets
positive_count = df_target[df_target['target'] == 1]['count'].values
negative_count = df_target[df_target['target'] == 0]['count'].values

# Determine which value represents positive tweets based on counts
if len(positive_count) > 0 and (len(negative_count) == 0 or positive_count[0] > negative_count[0]):
    positive_value = 0
    negative_value = 1
elif len(negative_count) > 0:
    positive_value = 1
    negative_value = 0
else:
    # Handle the case where both counts are empty
    positive_value = None
    negative_value = None

if positive_value is not None and negative_value is not None:
    print(f"Assuming {df_target['count'].sum()} tweets:")
    print(f"Value {positive_value} represents positive tweets.")
    print(f"Value {negative_value} represents negative tweets.")
else:
    print("Unable to determine which value represents positive or negative tweets.")

import matplotlib.pyplot as plt
df_target = (
    df[0].value_counts().to_frame(name='count').reset_index()
    .rename(columns={'index': 0})
)

# Convert 'target' to numeric (1 for positive, 0 for negative)
df_target[0] = df_target[0].replace(4, 1)

# Calculate user counts for positive and negative tweets
positive_tweets = df_target[df_target[0] == 1]['count'].sum()
negative_tweets = df_target[df_target[0] == 0]['count'].sum()

#sunplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 6))

# Pie Chart for Sentiment Distribution
ax1.pie(
    [positive_tweets, negative_tweets],
    labels=['Positive', 'Negative'],
    autopct="%1.1f%%",
    startangle=90,
    colors=['green', 'red'],
    wedgeprops=dict(width=0.6)
)
ax1.set_title('Sentiment Distribution')

# Bar Chart for User Counts
ax2.bar(['Positive', 'Negative'], [positive_tweets, negative_tweets], color=['green', 'red'])
ax2.set_xlabel('Sentiment')
ax2.set_ylabel('Count')
ax2.set_title('Sentiments')

# Tight layout for better overall plot arrangement
plt.tight_layout()

# Display the combined plot
plt.show()

# DATA CLEANING
# 1. remove HTML tags and URLS
# 2. lowercase all words
# 3. translate slang words
# 4. remove punctuation
# 5. remove StopWords (a , I, of, for etc)
import re

# Function to remove HTML tags
def remove_html_tags(text):
    clean_text = re.sub(r'<.*?>', '', text)
    return clean_text

# Function to remove URLs
def remove_urls(text):
    clean_text = re.sub(r'http\S+', '', text)
    return clean_text

    import string

# Function to convert text to lowercase
def convert_to_lowercase(text):
    return text.lower()

# Function to replace chat words
def replace_chat_words(text):
    chat_words = {
        "BRB": "Be right back",
        "BTW": "By the way",
        "OMG": "Oh my God/goodness",
        "TTYL": "Talk to you later",
        "OMW": "On my way",
        "SMH/SMDH": "Shaking my head/shaking my darn head",
        "LOL": "Laugh out loud",
        "TBD": "To be determined",
        "IMHO/IMO": "In my humble opinion",
        "HMU": "Hit me up",
        "IIRC": "If I remember correctly",
        "LMK": "Let me know",
        "OG": "Original gangsters (used for old friends)",
        "FTW": "For the win",
        "NVM": "Nevermind",
        "OOTD": "Outfit of the day",
        "Ngl": "Not gonna lie",
        "Rq": "real quick",
        "Iykyk": "If you know, you know",
        "Ong": "On god (I swear)",
        "YAAAS": "Yes!",
        "Brt": "Be right there",
        "Sm": "So much",
        "Ig": "I guess",
        "Wya": "Where you at",
        "Istg": "I swear to god",
        "Hbu": "How about you",
        "Atm": "At the moment",
        "Asap": "As soon as possible",
        "Fyi": "For your information"
    }

    for word, expanded_form in chat_words.items():
        text = text.replace(word, expanded_form)
    return text

import string,time
string.punctuation

from nltk.corpus import stopwords

# Function to remove punctuation
def remove_punctuation(text):
    clean_text = ''.join(ch for ch in text if ch not in string.punctuation)
    return clean_text

# Function to remove stopwords
def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return ' '.join(filtered_words)
def remove_whitespace(text):
    return text.strip()

# Function to remove special characters
def remove_special_characters(text):
    clean_text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    return clean_text
def preprocess_text(text):
    text = remove_html_tags(text)
    text = remove_urls(text)
    text = convert_to_lowercase(text)
    text = replace_chat_words(text)
    text = remove_punctuation(text)
    # text = remove_stopwords(text)
    text = remove_whitespace(text)
    text = remove_special_characters(text)
    return text

# Apply preprocessing function to DataFrame
df[5] = df[5].apply(preprocess_text)

# word frequency plot
# Function to plot bar plot of word frequencies
def plot_word_frequencies(text, title):
    word_freq = nltk.FreqDist(text.split())
    common_words = word_freq.most_common(20)
    words, freqs = zip(*common_words)
    plt.figure(figsize=(10, 6))
    sns.barplot(x=list(words), y=list(freqs), palette='viridis')
    plt.title(title)
    plt.xlabel('Words')
    plt.ylabel('Frequency')
    plt.xticks(rotation=45)
    plt.show()
# Step 3: Plot bar plots of word frequencies
plot_word_frequencies(' '.join(df[5]), 'Word Frequencies After Cleaning')
# plot_word_frequencies(' '.join(df[4]), 'User Word Frequencies After Cleaning')
# plot_word_frequencies(' '.join(df[3]), 'Flag Word Frequencies After Cleaning')

# prove there are no missing values in the dataset
#Index(['target', 'ids', 'date', 'flag', 'user', 'text'], dtype='object')
df.isnull().sum().sort_values(ascending=False)

# TRAIN LINEAR REGRESSION MODEL

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# convert tokenized text to BoW features
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df[5])

# assuming df is your DataFrame with the target column modified
y = df[0]

# split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# initialize and train logistic regression model
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

# predict on the testing set
y_pred = logreg.predict(X_test)

# eval model
accuracy = accuracy_score(y_test, y_pred)
print("Logistic Regression Accuracy:", accuracy)

